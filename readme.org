# -*- org-confirm-babel-evaluate: nil; -*-
#+title: Portfolio
#+subtitle: Machine Learning
#+author: Brian Brunner
#+description: Final project for CSC 448: Machine Learning

#+options: html5-fancy:t
#+html_doctype: html5
#+setupfile: ./org-html-themes/org/theme-readtheorg-local.setup

The inline blocks in this document are executed via org-babel to produce this site and output, and are representative of an interactive session. The source of the project and this document are available on my github [[https://github.com/Briaoeuidhtns/Machine-Learning][here]].

* Setup
Install the project dependencies via Poetry
#+begin_src bash :exports both :results pp
  poetry install
#+end_src

Activate the venv. In emacs you can activate a venv located at =.venv= via
#+begin_src emacs-lisp :session main :exports both
  (pyvenv-activate ".venv")
#+end_src

Load and shape some data for demonstrations
#+begin_src python :session main :exports both
  import numpy as np
  from portfolio.visualize import Plot

  from sklearn.datasets import load_iris
  iris = load_iris()

  # for binary classifications, get data for only type 1 and 2
  x = iris.data[iris.target > 0]
  # label as -1 and 1
  y = np.where(iris.target[iris.target > 0] == 1, -1, 1)
#+end_src

* TODO Perceptron

* Linear Regression
For single dimension data, linear regression is defined as $w = A^{-1}b$, where A is $xx^\mathsf{T}$, and b is $yx$. Since the matrix may not be invertible, we take the pseudo inverse. Since I added a column of ones to the A matrix, $w_0$ holds the y intercept, and since it is only a single dimension, $w_1$ holds the slope of the line. The linear regression code may be run on the iris dataset with the following:
#+begin_src python :session main :exports both :results verbatim html
  from portfolio.linear_regression import LinearRegression

  x_sepal_width = x[:, 1]

  regression_plot = Plot(x_sepal_width, y)
  regression_plot.add_view(LinearRegression)
  regression_plot.embed()

#+end_src

* Decision Stumps
Decision stumps are a type of weak learner. They consist of a decision tree with only a single node. My implementation only accepts a binary split of classes labeled -1 and 1, although in theory a decision stump can split based on threshold into any number of classes.

I couldn't figure out how to translate the pseudocode for the efficient $O(dm)$ implementation, so this is the na√Øve $O(dm^2)$ version. I also have a somewhat fuzzy understanding of the book's explanation, but this is equivalent as far as I can tell.

Since the decision stump needs a threshold $\theta$ in order to classify everything on each side as a different class, we need to define a set of steps. Given that two points may be arbitrarily close, I used the $x$ values from the data to ensure that if there was a single optimal $\theta$ it would be selected.

The other part of the decision stump is the dimension of the data against which it will classify. This implementation considers all given dimensions and selects the most suitable one.

The last part is the error function. The error function I used is a count of the differences between the predicted class and the expected class.

For each of the combinations of dimensions and possible thresholds, the decision stump checks the error function. The one with the lowest error is selected as the dimension to classify against and the $\theta$ to use as a threshold.
